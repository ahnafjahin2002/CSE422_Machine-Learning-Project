# -*- coding: utf-8 -*-
"""final_422_project_.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RyOCIve_5Cba9om1p9ZCzuk0q1zyse8f
"""

# CSE422 Lab Project

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout


# 1. Load Dataset
df = pd.read_csv("Customer_Category_Classifier_Dataset.csv")

print("Dataset Shape:", df.shape)
print(df.head())


# 2. Preprocessing
df = df.drop("ID", axis=1)

# Fill missing values (numerical → mean, categorical → mode)
for col in df.columns:
    if df[col].dtype == "object":
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())


# 2.1 Exploratory Data Analysis (EDA)
print("\n--- Starting Exploratory Data Analysis (EDA) ---")

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Ever_Married', y='Age', palette='viridis')
plt.title('Age Distribution by Marital Status')
plt.show()

plt.figure(figsize=(12, 7))
sns.countplot(data=df, x='Profession', hue='Ever_Married', palette='plasma')
plt.title('Profession Distribution Across Marital Status')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Spending_Score', hue='Ever_Married', palette='magma')
plt.title('Spending Score Distribution by Marital Status')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='Gender', hue='Ever_Married', palette='cividis')
plt.title('Gender Distribution by Marital Status')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='Age', bins=30, kde=True, color='teal')
plt.title('Overall Distribution of Customer Age')
plt.show()


# 2.2 Continue Preprocessing

plt.figure(figsize=(6,4))
sns.countplot(x="Ever_Married", data=df, palette="viridis")
plt.title("Class Distribution (Ever_Married)")
plt.show()

# Encode target
y = df["Ever_Married"]
le = LabelEncoder()
y = le.fit_transform(y)  # Yes=1, No=0

# Separate features
X = df.drop("Ever_Married", axis=1)
X = pd.get_dummies(X, drop_first=True)

# --- REMOVE UNNECESSARY FEATURES ---

# 1. Drop features with near-zero variance
low_var_cols = X.columns[X.std() < 0.05]
X = X.drop(columns=low_var_cols)
print("\nDropped low-variance features:", list(low_var_cols))

# 2. Drop features with very low correlation with target
corrs = pd.concat([pd.DataFrame(X), pd.Series(y, name="Target")], axis=1).corr()["Target"]
low_corr_cols = corrs[abs(corrs) < 0.05].index.tolist()
low_corr_cols = [c for c in low_corr_cols if c != "Target"]
X = X.drop(columns=low_corr_cols)
print("\nDropped low-correlation features:", low_corr_cols)

# Correlation heatmap
plt.figure(figsize=(12,8))
sns.heatmap(pd.DataFrame(X).corr(), cmap="coolwarm")
plt.title("Feature Correlation Heatmap (After Cleaning)")
plt.show()

# Scale features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)


# 3. KNN Classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)
knn_acc = accuracy_score(y_test, y_pred_knn)

print("\n--- KNN ---")
print("Accuracy:", knn_acc)
print(classification_report(y_test, y_pred_knn))

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_knn), annot=True, cmap="Blues", fmt="d")
plt.title("KNN Confusion Matrix")
plt.show()


# 4. Decision Tree
dt = DecisionTreeClassifier(random_state=42, max_depth=6)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
dt_acc = accuracy_score(y_test, y_pred_dt)

print("\n--- Decision Tree ---")
print("Accuracy:", dt_acc)
print(classification_report(y_test, y_pred_dt))

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, cmap="Greens", fmt="d")
plt.title("Decision Tree Confusion Matrix")
plt.show()


# 5. Neural Network (Adam + ReLU)
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=50, batch_size=32, verbose=1
)

loss, nn_acc = model.evaluate(X_test, y_test, verbose=0)
print("\n--- Neural Network (Adam + ReLU) ---")
print("Accuracy:", nn_acc)

y_pred_nn = (model.predict(X_test) > 0.5).astype("int32")
plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_nn), annot=True, cmap="Reds", fmt="d")
plt.title("Neural Network Confusion Matrix (Adam + ReLU)")
plt.show()


# 6. ROC Curve
y_score_knn = knn.predict_proba(X_test)[:,1]
y_score_dt = dt.predict_proba(X_test)[:,1]
y_score_nn = model.predict(X_test).ravel()

plt.figure(figsize=(8,6))
for model_name, y_score in zip(["KNN", "Decision Tree", "Neural Net"], [y_score_knn, y_score_dt, y_score_nn]):
    fpr, tpr, _ = roc_curve(y_test, y_score)
    plt.plot(fpr, tpr, label=f"{model_name} (AUC={auc(fpr,tpr):.2f})")

plt.plot([0,1],[0,1],'k--')
plt.title("ROC Curve (Binary Target: Ever_Married)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.show()


# 7. Accuracy Comparison
acc_scores = {
    "KNN": knn_acc,
    "Decision Tree": dt_acc,
    "Neural Network (Adam+ReLU)": nn_acc
}

plt.bar(acc_scores.keys(), acc_scores.values(), color=["blue","green","red"])
plt.title("Model Comparison (Accuracy)")
plt.ylabel("Accuracy")
plt.show()


# 8. Precision, Recall, F1-score Comparison
prec_knn = precision_score(y_test, y_pred_knn)
rec_knn = recall_score(y_test, y_pred_knn)
f1_knn  = f1_score(y_test, y_pred_knn)

prec_dt = precision_score(y_test, y_pred_dt)
rec_dt = recall_score(y_test, y_pred_dt)
f1_dt  = f1_score(y_test, y_pred_dt)

prec_nn = precision_score(y_test, y_pred_nn)
rec_nn = recall_score(y_test, y_pred_nn)
f1_nn  = f1_score(y_test, y_pred_nn)

metrics = {
    "KNN": [prec_knn, rec_knn, f1_knn],
    "Decision Tree": [prec_dt, rec_dt, f1_dt],
    "Neural Network (Adam+ReLU)": [prec_nn, rec_nn, f1_nn]
}

metrics_df = pd.DataFrame(metrics, index=["Precision", "Recall", "F1-score"])
metrics_df.T.plot(kind="bar", figsize=(10,7))
plt.title("Precision, Recall, and F1-score Comparison")
plt.ylim(0,1)
plt.show()


# 9. Unsupervised Learning: K-Means Clustering
print("\n--- Unsupervised Learning: K-Means Clustering ---")

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42, n_init=10)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42, n_init=10)
y_kmeans = kmeans.fit_predict(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['Cluster'] = y_kmeans

plt.figure(figsize=(10, 7))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_pca, palette='viridis', alpha=0.7)
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroids')
plt.title('K-Means Clusters of Customer Data (PCA Visualized)')
plt.legend()
plt.show()